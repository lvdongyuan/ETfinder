#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
TaxID â†’ ç‰©ç§æ ‘æ„å»ºä¸å¸¦æ³¨é‡Šå¶æ ‡ç­¾æ¸²æŸ“ï¼ˆå…¼å®¹ CL å‰ç¼€ï¼›å®‰å…¨æ–‡æœ¬æ›¿æ¢ï¼Œä¸ç”¨è§£æå™¨å†™å›ï¼‰
- ä» index.tsv è¯»å–ï¼šID(=CL{cid}-{num}), Strain, TaxID, Status, Role, Similarity
- æ„å»º NCBI ç‰©ç§æ ‘ï¼ˆå¶å=taxidï¼‰ï¼Œå†æŠŠå¶å­çš„ taxid æ–‡æœ¬æ›¿æ¢ä¸ºï¼š
    CL3-1|'StrainA'[&status=1,c=rep]_'StrainB'[&status=0,c=97.3]_'+2'
- é¢å¤–çš„â€œmember-only / é¢å¤– rep çš„ CL å—â€æŠ˜å è¿›å¤–å±‚æ³¨é‡Šå±æ€§ï¼š
    [&member_clusters_count=..., member_clusters='...']
    [&extra_rep_clusters_count=..., extra_rep_clusters='...']
  æŠ˜å æ–‡æœ¬å†…éƒ¨çš„æ³¨é‡Šæ–¹æ‹¬å·ä¼šè¢«è½¬æˆå…¨è§’ï¼»ï¼½ï¼Œä»¥ä¾¿**ä¿ç•™ status/c å†…å®¹**ä¸”ä¸å¹²æ‰° Newick è§£æã€‚
"""

import os
import re
from collections import defaultdict, OrderedDict
from typing import Dict, List, Tuple, Optional
from ete3 import NCBITaxa, Tree

# ====== ä¿®æ”¹è¿™é‡Œ ======
INDEX_TSV   = "/Users/lvdongyuan/Desktop/Bioinformatics/ETfinder/analyze_ssap_ssb/ETfinder2/index.tsv"
OUT_TREE    = "/Users/lvdongyuan/Desktop/Bioinformatics/ETfinder/analyze_ssap_ssb/ETfinder2/tree/taxid_tree.nwk"
OUT_LABELED = "/Users/lvdongyuan/Desktop/Bioinformatics/ETfinder/analyze_ssap_ssb/ETfinder2/tree/taxid_tree.labeled.nwk"

# ç²¾ç®€é”šç‚¹ï¼ˆå¯é€‰ï¼‰
ANCHOR_TAXIDS = {
    562:    "â­Escherichia coli",
    155892: "â­Caulobacter crescentus",
    1718:   "â­Corynebacterium glutamicum",
    1902:   "â­Streptomyces coelicolor",
    817:    "â­Bacteroides fragilis",
    1423:   "â­Bacillus subtilis",
}

# â€”â€” ç”¨æˆ·è¾“å…¥ï¼ˆå¯ä¸ºç©ºï¼‰ï¼šç»™â€œç‰©ç§å/ä¿—å/ä¸‹åˆ’çº¿å½¢å¼/æˆ– taxid å­—ç¬¦ä¸²â€
USER_TARGET_INPUT: Optional[str] = None   # ä¾‹ï¼š"Caulobacter crescentus" æˆ– "155892"

# ç­–ç•¥
ANCHOR_POLICY = 'skip_if_in_index'   # æˆ– 'merge'
TARGET_POLICY = 'merge'              # æˆ– 'skip_if_in_index'

# å±•ç¤ºæ§åˆ¶
MAX_STRAINS_PER_CLUSTER = 3          # æ¯ä¸ª CL å±•ç¤ºçš„èŒæ ªæ¡ç›®ä¸Šé™
FALLBACK_SCIENTIFIC     = True       # æ‰¾ä¸åˆ°æ˜ å°„æ—¶å›é€€ä¸ºå­¦å

# æŠ˜å ç­–ç•¥
COLLAPSE_MEMBER_CLUSTERS_TO_ANNOT   = True   # ä»… member çš„ CL æ•´æ®µæŠ˜å è¿›æ³¨é‡Š
COLLAPSE_EXTRA_REP_CLUSTERS_TO_ANNOT = True  # é¢å¤– rep çš„ CL ä¹ŸæŠ˜å 

# ä»…ç”¨å« rep çš„ç‰©ç§å‚ä¸æ„æ ‘ï¼ˆå¦åˆ™ anchor/target ä¹Ÿä¼šè¢«çº³å…¥ï¼‰
USE_REP_ONLY_FOR_TREE = True

def distance_map_from_raw_nwk(nwk_path: str, target_taxid: int | None) -> dict[int, float]:
    """
    è¯»å– *åŸå§‹* ç‰©ç§æ ‘ï¼ˆå¶å=TaxID çš„ newickï¼‰ï¼Œè¿”å› target_taxid åˆ°æ‰€æœ‰å¶çš„æ ‘è·ã€‚
    ç›®æ ‡ä¸åœ¨æ ‘æˆ–ä¸º None â†’ è¿”å›ç©º dictã€‚
    """
    if not target_taxid:
        return {}
    t = Tree(nwk_path, format=1)
    leaves = {}
    for lf in t.iter_leaves():
        name = (lf.name or "").strip()
        if name.isdigit():
            leaves[int(name)] = lf
    if target_taxid not in leaves:
        return {}
    anchor = leaves[target_taxid]
    return {tid: t.get_distance(anchor, lf) for tid, lf in leaves.items()}

# ============ å°å·¥å…· ============
def ensure_parent_dir(p: str):
    d = os.path.dirname(os.path.abspath(p))
    if d:
        os.makedirs(d, exist_ok=True)

def quote_strain(s: str) -> str:
    """æŠŠèŒæ ªååŒ…æˆ Newick å®‰å…¨çš„ '...'ï¼ˆå†…éƒ¨å•å¼•å·è½¬ä¸ºå¼¯å¼•å·ï¼‰"""
    return "'" + str(s).replace("'", "â€™") + "'"

def sanitize_sim(sim: str) -> str:
    sim = (sim or "").strip()
    return re.sub(r"[^0-9.]", "", sim) if sim else ""

def quote_attr(s: str) -> str:
    """æŠŠæ³¨é‡Šé‡Œçš„å±æ€§å€¼åŒ…æˆ '...'ï¼Œå†…éƒ¨å•å¼•å·è½¬ä¸ºå¼¯å¼•å·ï¼Œå»æ‰æ¢è¡Œ"""
    if s is None:
        return "''"
    s = str(s).replace("'", "â€™").replace("\n", " ").strip()
    return f"'{s}'"

def neutralize_ann_for_attr(s: str) -> str:
    """
    æŠŠå†…å±‚ Newick æ³¨é‡Šå— `[&key=val,...]` è½¬å†™æˆèŠ±æ‹¬å· `{key=val,...}`ï¼Œ
    ä»¥ä¾¿ä½œä¸ºå±æ€§å€¼å®‰å…¨åµŒå…¥ï¼Œå¹¶ä¿ç•™å…¨éƒ¨å†…å®¹ã€‚
    åªåšæ‹¬å·å½¢çŠ¶æ›¿æ¢ï¼Œä¸åŠ¨å†…å®¹ä¸é€—å·/ç­‰å·ã€‚
    """
    if not s:
        return s
    t = s.replace("\n", " ").strip()
    # å•å±‚æ›¿æ¢ï¼šæŠŠæ¯ä¸ª `[& ... ]` å˜æˆ `{ ... }`
    return re.sub(r"\[\&([^\[\]]*)\]", r"{\1}", t)



def escape_ann_for_attr(s: str) -> str:
    """
    æŠ˜å åˆ°å±æ€§å€¼é‡Œçš„â€œå†…å±‚æ–‡æœ¬å—â€å®‰å…¨åŒ–ï¼š
    - **ä¿ç•™**å†…éƒ¨ [&status=...,c=...] å†…å®¹
    - ä»…æŠŠ '[' ']' æ›¿æ¢ä¸ºå…¨è§’ï¼Œé¿å…å¤–å±‚æ³¨é‡Šè¯¯è§£æ
    - æ¸…ç†æ¢è¡Œä¸é¦–å°¾ç©ºç™½
    """
    if not s:
        return s
    t = s.replace("\n", " ").strip()
    return t.replace("[", "ï¼»").replace("]", "ï¼½")

# ============ è¯»å– index.tsv ============
def load_taxid_to_entries(index_file: str) -> Dict[str, OrderedDict]:
    """
    TaxID(str) -> OrderedDict(
        ID(str: CL{cid}-{num}) -> list[(strain, status, role, sim)]
    )
    """
    with open(index_file, "r", encoding="utf-8") as f:
        header = f.readline().rstrip("\n").split("\t")
        cols = {name: i for i, name in enumerate(header)}
        for k in ("ID", "Strain", "TaxID", "Status"):
            if k not in cols:
                raise RuntimeError(f"index.tsv ç¼ºå°‘åˆ—ï¼š{k}")
        has_role = "Role" in cols
        has_sim  = "Similarity" in cols

        mapping: Dict[str, OrderedDict] = defaultdict(OrderedDict)
        for line in f:
            if not line.strip():
                continue
            parts = line.rstrip("\n").split("\t")
            try:
                taxid = str(int(parts[cols["TaxID"]]))
            except ValueError:
                continue

            cid    = parts[cols["ID"]].strip()      # e.g., CL3-1
            strain = parts[cols["Strain"]].strip()
            status = parts[cols["Status"]].strip()
            role   = parts[cols["Role"]].strip() if has_role else ""
            sim    = parts[cols["Similarity"]].strip() if has_sim else ""

            if not cid:
                continue
            if cid not in mapping[taxid]:
                mapping[taxid][cid] = []
            quad = (strain, status, role, sim)
            if strain and quad not in mapping[taxid][cid]:
                mapping[taxid][cid].append(quad)
    return mapping

# ============ æ³¨å…¥ anchor / target ============
def inject_anchors_and_target(mapping: Dict[str, OrderedDict],
                              user_taxid: Optional[int]) -> Dict[str, OrderedDict]:
    ncbi = NCBITaxa()
    anchor_map = dict(ANCHOR_TAXIDS)

    # è‹¥ç”¨æˆ·ç›®æ ‡ä¸é”šç‚¹é‡åˆï¼šé”šç‚¹è®©è·¯
    if user_taxid is not None:
        anchor_map.pop(int(user_taxid), None)

    # æ³¨å…¥é”šç‚¹ï¼ˆID=ç‰©ç§åï¼Œrole=anchorï¼‰
    for tid, label in anchor_map.items():
        s_tid = str(int(tid))
        already = s_tid in mapping and len(mapping[s_tid]) > 0
        if already and ANCHOR_POLICY == 'skip_if_in_index':
            continue
        if s_tid not in mapping:
            mapping[s_tid] = OrderedDict()
        if label not in mapping[s_tid]:
            mapping[s_tid][label] = []
        quad = ("", "", "anchor", "")
        if quad not in mapping[s_tid][label]:
            mapping[s_tid][label].append(quad)

    # æ³¨å…¥ç›®æ ‡ï¼ˆID=ğŸ¯Speciesï¼Œrole=targetï¼‰
    if user_taxid is not None:
        s_tid = str(int(user_taxid))
        try:
            sci = ncbi.get_taxid_translator([int(user_taxid)])[int(user_taxid)]
            label = f"ğŸ¯{sci}"
        except Exception:
            label = f"ğŸ¯taxid_{user_taxid}"

        already = s_tid in mapping and len(mapping[s_tid]) > 0
        if not (already and TARGET_POLICY == 'skip_if_in_index'):
            if s_tid not in mapping:
                mapping[s_tid] = OrderedDict()
            if label not in mapping[s_tid]:
                mapping[s_tid][label] = []
            quad = ("", "", "target", "")
            if quad not in mapping[s_tid][label]:
                mapping[s_tid][label].append(quad)

    return mapping

# ============ é€‰æ‹©æ„æ ‘ taxid ============
def collect_taxids_for_tree(mapping: Dict[str, OrderedDict],
                            rep_only: bool) -> List[int]:
    selected = set()
    for s_tid, entry_dict in mapping.items():
        roles = [ (r[2] or "").lower() for rows in entry_dict.values() for r in rows ]
        has_anchor_target = any(x in ("anchor","target") for x in roles)
        has_rep = any(x == "rep" for x in roles)
        if (not rep_only) or has_rep or has_anchor_target:
            try:
                selected.add(int(s_tid))
            except ValueError:
                pass
    return sorted(selected)

# ============ æ„åŸå§‹æ ‘ ============
def build_tree_from_taxids(taxids: List[int], out_file: str):
    ensure_parent_dir(out_file)
    ncbi = NCBITaxa()
    tree = ncbi.get_topology(sorted(taxids))
    tree.write(format=1, outfile=out_file)
    print(f"[INFO] åŸå§‹ç‰©ç§æ ‘å·²ä¿å­˜: {out_file}ï¼ˆtips={len(taxids)}ï¼‰")

# ============ æ¸²æŸ“å•ä¸ª TaxID çš„å¶æ ‡ç­¾ ============
def build_label_for_taxid(entry_dict: OrderedDict,
                          max_strains: int = MAX_STRAINS_PER_CLUSTER) -> str:
    """
    ç”Ÿæˆå•ä¸ª taxid çš„å¶æ ‡ç­¾ï¼ˆæ”¯æŒæŠ˜å â€œmember-onlyâ€ä¸â€œé¢å¤– repâ€çš„ CL å—ï¼‰ã€‚
    æŠ˜å æ–‡æœ¬é‡Œçš„å†…éƒ¨æ³¨é‡Šä» `[&key=val,...]` è½¬å†™ä¸º `{key=val,...}`ï¼Œå†…å®¹ä¿ç•™ï¼Œé¿å…ç ´åå¤–å±‚ Newick æ³¨é‡Šã€‚
    è¿”å›ç¤ºä¾‹ï¼š
      CL3-1|'StrainA'[&status=1,c=rep]_'StrainB'[&status=0,c=97.3]_'+2'
        [&member_clusters_count=1,member_clusters='CL8-0|â€™Sâ€™{status=0,c=91.2} || ...']
        [&extra_rep_clusters_count=1,extra_rep_clusters='CL9-0|â€™Sâ€™{status=1,c=rep}']
    """
    # --- æŠŠå†…å±‚æ³¨é‡Š `[&...]` è½¬å†™æˆ `{...}`ï¼Œä»…æ”¹æ‹¬å·å½¢çŠ¶ï¼Œå†…å®¹ä¸€å­—ä¸ä¸¢ ---
    def neutralize_ann_for_attr(s: str) -> str:
        if not s:
            return s
        t = s.replace("\n", " ").strip()
        return re.sub(r"\[\&([^\[\]]*)\]", r"{\1}", t)

    # 1) åˆ†ç¦» anchor/targetï¼ˆæ€»æ˜¯å¯è§ï¼‰ä¸æ™®é€š CL
    anchor_chunks = []
    normal_clusters = []
    for ident, rows in entry_dict.items():
        roles = {(r[2] or "").lower() for r in rows}
        if "anchor" in roles or "target" in roles:
            cval = "anchor" if "anchor" in roles else "target"
            anchor_chunks.append(f"{ident}[&c={cval}]")
        else:
            normal_clusters.append((ident, rows))

    # 2) CL æ’åºï¼šå« rep > æœ€é«˜ç›¸ä¼¼åº¦ > æ¡ç›®æ•°
    def cluster_score(rows):
        has_rep = any((r[2] or "").lower() == "rep" for r in rows)
        best_sim = max(
            (float(sanitize_sim(r[3])) if sanitize_sim(r[3]) else -1.0) for r in rows
        ) if rows else -1.0
        return (1 if has_rep else 0, best_sim, len(rows))

    normal_clusters_sorted = sorted(
        normal_clusters, key=lambda x: cluster_score(x[1]), reverse=True
    )

    # 3) æ¸²æŸ“å•ä¸ª CL æ–‡æœ¬å—
    def render_cluster_block(cid: str, rows: list) -> tuple[str, bool]:
        """è¿”å› (block_text, has_rep_in_this_taxid)"""
        def score_row(r):
            status, role, sim = r[1], (r[2] or "").lower(), r[3]
            is_rep = 1 if role == "rep" else 0
            s1 = 1 if str(status) == "1" else 0
            try:
                simv = float(sanitize_sim(sim)) if sanitize_sim(sim) else -1.0
            except Exception:
                simv = -1.0
            return (is_rep, s1, simv)

        rows_sorted = sorted(rows, key=score_row, reverse=True)
        shown = rows_sorted[:max_strains] if (max_strains and max_strains > 0) else rows_sorted
        hidden_rows = rows_sorted[len(shown):]

        bits = []
        has_rep_here = any((r[2] or "").lower() == "rep" for r in rows)
        for s, st, role, sim in shown:
            if not s:
                continue
            attrs = []
            if st != "":
                attrs.append(f"status={st}")
            if (role or "").lower() == "rep":
                attrs.append("c=rep")
            else:
                sim_clean = sanitize_sim(sim)
                attrs.append(f"c={sim_clean}" if sim_clean else "c=member")
            bits.append(f"{quote_strain(s)}[&{','.join(attrs)}]")

        if hidden_rows:
            hidden_names = [h[0] for h in hidden_rows if h and h[0]]
            bits.append(
                f"{quote_strain(f'+{len(hidden_rows)}')}[&fold=strain,hidden={quote_attr('|'.join(hidden_names))}]"
            )

        strain_part = "_".join(bits) if bits else "''"
        return f"{cid}|{strain_part}", has_rep_here

    # 4) æ¸²æŸ“æ‰€æœ‰æ™®é€š CLï¼ŒåŒºåˆ†å« rep ä¸ member-only
    rendered = [(cid,) + render_cluster_block(cid, rows)
                for cid, rows in normal_clusters_sorted]
    visible_rep_blocks   = [text for cid, text, has_rep in rendered if has_rep]
    hidden_member_blocks = [text for cid, text, has_rep in rendered if not has_rep] \
                           if COLLAPSE_MEMBER_CLUSTERS_TO_ANNOT else []

    # 5) ç»„è£…ï¼šanchor/target åœ¨å‰ï¼Œä¸»å—ä¸ºç¬¬ä¸€ä¸ªå« rep çš„ CLï¼Œå…¶å®ƒæŠ˜å è¿›æ³¨é‡Š
    chunks: List[str] = []
    chunks.extend(anchor_chunks)

    if visible_rep_blocks:
        first = visible_rep_blocks[0]

        if hidden_member_blocks:
            merged_hidden = neutralize_ann_for_attr(" || ".join(hidden_member_blocks))
            first = (
                f"{first}[&member_clusters_count={len(hidden_member_blocks)},"
                f"member_clusters={quote_attr(merged_hidden)}]"
            )

        extra_reps = visible_rep_blocks[1:]
        if extra_reps:
            if COLLAPSE_EXTRA_REP_CLUSTERS_TO_ANNOT:
                merged_extra = neutralize_ann_for_attr(" || ".join(extra_reps))
                first = (
                    f"{first}[&extra_rep_clusters_count={len(extra_reps)},"
                    f"extra_rep_clusters={quote_attr(merged_extra)}]"
                )
                chunks.append(first)
            else:
                chunks.append(first)
                chunks.extend(extra_reps)
        else:
            chunks.append(first)
    else:
        # æ²¡æœ‰ repï¼šå…œåº•å±•ç¤ºç¬¬ä¸€å—
        if rendered:
            chunks.append(rendered[0][1])

    # anchor/target å’Œå¯è§å—ä¹‹é—´ç”¨ " || " åˆ†éš”
    return " || ".join(chunks)

# ============ æ–‡æœ¬çº§ä»…æ›¿æ¢â€œå¶å­ taxidâ€ ============
# åªåŒ¹é…å¶å­ä½ç½®çš„ tokenï¼š
#  group(1) = åˆ†éš”ç¬¦ï¼ˆè¡Œé¦–"" æˆ– '(' æˆ– ','ï¼‰ï¼Œä¿ç•™åœ¨è¾“å‡ºä¸­
#  group(2) = å¶å­åï¼ˆè¿™é‡Œæ˜¯ taxid æ•°å­—ï¼‰
LEAF_TOKEN_PAT = re.compile(r'(^|[\(,])(\d+)(?=[:),])')

def relabel_leaves_in_text(nwk_in: str,
                           nwk_out: str,
                           taxid_to_entries: Dict[str, OrderedDict],
                           max_strains: int = MAX_STRAINS_PER_CLUSTER,
                           fallback_scientific: bool = True):
    ensure_parent_dir(nwk_out)
    with open(nwk_in, "r", encoding="utf-8") as f:
        nwk_txt = f.read()

    # å…ˆæ”¶é›†æ‰€æœ‰å¶å­ taxidï¼Œä¾¿äºä¸€æ¬¡æ€§ç¿»è¯‘å­¦åï¼ˆå¯é€‰ï¼‰
    all_taxid_tokens = {m.group(2) for m in LEAF_TOKEN_PAT.finditer(nwk_txt)}
    sci = {}
    if fallback_scientific and all_taxid_tokens:
        ints = [int(x) for x in all_taxid_tokens if x.isdigit()]
        if ints:
            try:
                sci = NCBITaxa().get_taxid_translator(ints)
            except Exception:
                sci = {}

    known_ids = set(taxid_to_entries.keys())
    mapped = 0
    missed_ids = set()

    def _repl(m: re.Match) -> str:
        nonlocal mapped
        prefix = m.group(1)
        tid    = m.group(2)   # string
        if tid in known_ids:
            label = build_label_for_taxid(taxid_to_entries[tid], max_strains=max_strains)
            mapped += 1
            return prefix + label
        else:
            # å›é€€æ˜¾ç¤ºå­¦å
            try:
                name = sci.get(int(tid))
            except Exception:
                name = None
            if name:
                return prefix + name
            missed_ids.add(tid)
            return prefix + tid

    labeled_txt = LEAF_TOKEN_PAT.sub(_repl, nwk_txt)
    with open(nwk_out, "w", encoding="utf-8") as f:
        f.write(labeled_txt)

    print(f"[OK] æ›¿æ¢å®Œæˆï¼š{nwk_out} (mapped={mapped}, missed={len(missed_ids)})")
    if missed_ids:
        example = ", ".join(list(missed_ids)[:8])
        more = f" ...(+{len(missed_ids)-8})" if len(missed_ids) > 8 else ""
        print(f"[NOTE] è¿™äº› taxid æœªå‡ºç°åœ¨ index.tsvï¼š{example}{more}")

# ============ ä¸»ç¨‹åº ============
if __name__ == "__main__":
    # è§£æç”¨æˆ·ç›®æ ‡ï¼ˆå¯é€‰ï¼‰
    try:
        from sp2taxid import resolve_to_taxid  # ä½ æœ¬åœ°çš„å°å·¥å…·
    except Exception:
        resolve_to_taxid = lambda s: None

    user_tid = resolve_to_taxid(USER_TARGET_INPUT) if USER_TARGET_INPUT else None
    if USER_TARGET_INPUT and not user_tid:
        print(f"[WARN] æ— æ³•è§£æç”¨æˆ·è¾“å…¥ï¼š{USER_TARGET_INPUT}ï¼›å°†ä¸åŠ å…¥ç›®æ ‡ç‰©ç§ã€‚")

    # Step 1: è¯»å– index.tsv
    print("[STEP] è¯»å– index.tsv â€¦")
    taxid_to_entries = load_taxid_to_entries(INDEX_TSV)

    # Step 2: æ³¨å…¥ anchor / target
    print("[STEP] æ³¨å…¥ anchor/target â€¦")
    taxid_to_entries = inject_anchors_and_target(taxid_to_entries, user_tid)

    # Step 3: é€‰å–å‚ä¸æ„æ ‘çš„ taxid
    print("[STEP] é€‰æ‹©æ„æ ‘ taxid â€¦")
    taxids_for_tree = collect_taxids_for_tree(taxid_to_entries, rep_only=USE_REP_ONLY_FOR_TREE)
    print(f"[INFO] çº³å…¥æ„æ ‘çš„ taxid æ•°ï¼š{len(taxids_for_tree)} | æ¨¡å¼ï¼š{'rep-only' if USE_REP_ONLY_FOR_TREE else 'all'}")

    # Step 4: æ„å»ºåŸå§‹ Newickï¼ˆå¶å=taxidï¼‰
    print("[STEP] æ„å»ºåŸå§‹ Newick â€¦")
    build_tree_from_taxids(taxids_for_tree, OUT_TREE)

    # Step 5: æ–‡æœ¬çº§æ›¿æ¢ taxid â†’ å¸¦æ³¨é‡Šæ ‡ç­¾
    print("[STEP] æ¸²æŸ“å¸¦æ³¨é‡Šå¶æ ‡ç­¾ â€¦")
    relabel_leaves_in_text(
        nwk_in=OUT_TREE,
        nwk_out=OUT_LABELED,
        taxid_to_entries=taxid_to_entries,
        max_strains=MAX_STRAINS_PER_CLUSTER,
        fallback_scientific=FALLBACK_SCIENTIFIC
    )

    print("[DONE] åŸå§‹æ ‘:", OUT_TREE)
    print("[DONE] æ›¿æ¢å:", OUT_LABELED)